{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Model Building\n",
    "\n",
    "Here you try your hand at model building to predict appointment no shows.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Package 'proj2_lib' now includes code to carry out preprocessing steps from part I. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import proj2_lib.util as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it includes a dictionary used for configuring path and file names\n",
    "used through the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_pipeline_file': 'feature_pipeline.pkl',\n",
       " 'labels_pipeline_file': 'labels_pipeline.pkl',\n",
       " 'objstore_path': 'objects',\n",
       " 'processed_data_path': 'processed_data',\n",
       " 'raw_data_csv': 'KaggleV2-May-2016.csv',\n",
       " 'raw_data_path': 'data',\n",
       " 'test_csv': 'test_set.csv',\n",
       " 'train_csv': 'train_set.csv'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.file_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_pipeline_file`: file storing the preprocessing pipeline used for preparing the feature matrix\n",
    "\n",
    "`labels_pipeline_file`: file storing the preprocessing pipeline used for\n",
    "preparing labels\n",
    "\n",
    "`objstore_path`: directory to store python objects to disk\n",
    "\n",
    "`processed_data_path`: directory containing processed data\n",
    "\n",
    "`raw_data_csv`: name of the csv download from Kaggle\n",
    "\n",
    "`raw_data_path`: directory containing raw data\n",
    "\n",
    "`test_csv`: name of csv file containing test set\n",
    "\n",
    "`train_csv`: name of csv file containing train set\n",
    "\n",
    "You can change these paths and names to suit your project directory structure if you need so. E.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_config = utils.file_config\n",
    "#config['raw_data_path'] = \"some_other_directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create train test sets. Code is in file `proj2_lib/util.py` function `make_train_test_sets`. You\n",
    "can edit that function as needed to include your own part I code if you so desire. The result will be to \n",
    "create files `train_set.csv` and `test_set.csv` in your `processed_data` directory (unless you change any of the entries in the configuration directory as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY NEED TO RUN THIS STEP ONCE (switch this to True to run it)\n",
    "RUN_MAKE_TRAIN_TEST_FILES = False\n",
    "if RUN_MAKE_TRAIN_TEST_FILES:\n",
    "    utils.make_train_test_sets(config=file_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to fit the preprocessing pipelines. This is done in file `proj2_lib/preprocess.py`. Again you can edit code as needed in that file to incorporate your part I solution as you wish. The result will be to create files `feature_pipeline.pkl` and `labels_pipeline.pkl` containing the fit preprocessing pipelines we can then use to preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import proj2_lib.preprocess as preprocess\n",
    "\n",
    "# ONLY NEED TO RUN THIS STEP ONCE\n",
    "RUN_FIT_PREPROCESSING = True\n",
    "if RUN_FIT_PREPROCESSING:\n",
    "    preprocess.fit_save_pipelines(config=file_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once we do that, we can get a training matrix and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90526, 101)\n",
      "(90526,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "Using `sklearn` fit:\n",
    "    - DecisionTree classifier\n",
    "    - RandomForest classifier\n",
    "    - Linear SVM classifier\n",
    "    - SVM with Radial Basis Kernel classifier\n",
    "    \n",
    "Use default parameters for now.\n",
    "Using 10-fold cross validation report both accuracy and AUC for each of the above four models.\n",
    "\n",
    "QUESTION: Should you use accuracy or AUC for this task as a performance metric?\n",
    "\n",
    "_ANSWER HERE_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build your models here\n",
    "import numpy as np \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf_fitted = tree_clf.fit(train_X, train_y)\n",
    "\n",
    "#tree_predictions = tree_reg.predict(train_X)\n",
    "#accu = accuracy_score(train_y, tree_predictions.astype(int))\n",
    "#print(accu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier()\n",
    "forest_clf_fitted = forest_clf_fit=forest_clf.fit(train_X,train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVM_clf = LinearSVC()\n",
    "SVM_clf_fitted = SVM_clf_fit=SVM_clf.fit(train_X,train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "def measure_performance(X,y,clf, show_accuracy=True, show_AUC=True, show_classification_report=True, show_confusion_matrix=True):\n",
    "\n",
    "    accuracy_scores = cross_val_score(clf, X, y, \n",
    "                        scoring=\"accuracy\", cv=10)\n",
    "    \n",
    "    \n",
    "    AUC_scores = cross_val_score(clf, X, y, \n",
    "                        scoring=\"roc_auc\", cv=10)\n",
    "    \n",
    "    y_pred=clf.predict(X)   \n",
    "    \n",
    "    print (\"Fitted model:\")\n",
    "    print (clf,\"\\n\")\n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:\")\n",
    "        print (accuracy_scores,\"\\n\")\n",
    "        print (\"Mean Accuracy\")\n",
    "        print (accuracy_scores.mean(), \"\\n\")\n",
    "        \n",
    "    if show_AUC:\n",
    "        print (\"AUC:\")\n",
    "        print (AUC_scores,\"\\n\")        \n",
    "        print (\"Mean AUC\")\n",
    "        print (AUC_scores.mean(), \"\\n\")        \n",
    "        \n",
    "    if show_classification_report:\n",
    "        print (\"Classification report\")\n",
    "        print (metrics.classification_report(y,y_pred),\"\\n\")\n",
    "        \n",
    "    if show_confusion_matrix:\n",
    "        print (\"Confusion matrix\")\n",
    "        print (metrics.confusion_matrix(y,y_pred),\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best') \n",
      "\n",
      "Accuracy:\n",
      "[ 0.74351044  0.73853971  0.74130123  0.74008616  0.73445267  0.73798741\n",
      "  0.74060981  0.73298719  0.74171454  0.73442333] \n",
      "\n",
      "Mean Accuracy\n",
      "0.738561247486 \n",
      "\n",
      "AUC:\n",
      "[ 0.58489188  0.58400847  0.58266902  0.57965731  0.58077423  0.57645151\n",
      "  0.58484312  0.5759704   0.59008413  0.57904496] \n",
      "\n",
      "Mean AUC\n",
      "0.581839502662 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.98      1.00      0.99     72246\n",
      "          1       0.99      0.91      0.95     18280\n",
      "\n",
      "avg / total       0.98      0.98      0.98     90526\n",
      " \n",
      "\n",
      "Confusion matrix\n",
      "[[72107   139]\n",
      " [ 1622 16658]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decision tree \n",
    "measure_performance(train_X,train_y,tree_clf_fitted, show_classification_report=True, show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model:\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False) \n",
      "\n",
      "Accuracy:\n",
      "[ 0.78106705  0.77742185  0.77388711  0.77985198  0.77245112  0.77830553\n",
      "  0.77662395  0.77750773  0.77596111  0.77087936] \n",
      "\n",
      "Mean Accuracy\n",
      "0.776395680682 \n",
      "\n",
      "AUC:\n",
      "[ 0.69874486  0.69129091  0.68977096  0.70241064  0.69211894  0.69545933\n",
      "  0.69484385  0.68011177  0.69922749  0.68882718] \n",
      "\n",
      "Mean AUC\n",
      "0.693280593178 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.96      0.99      0.98     72246\n",
      "          1       0.96      0.84      0.90     18280\n",
      "\n",
      "avg / total       0.96      0.96      0.96     90526\n",
      " \n",
      "\n",
      "Confusion matrix\n",
      "[[71608   638]\n",
      " [ 2880 15400]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decision tree \n",
    "measure_performance(train_X,train_y,forest_clf_fitted, show_classification_report=True, show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model:\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) \n",
      "\n",
      "Accuracy:\n",
      "[ 0.79608969  0.79509555  0.79675246  0.79608969  0.79586877  0.796642\n",
      "  0.79595669  0.79750331  0.79639859  0.79617764] \n",
      "\n",
      "Mean Accuracy\n",
      "0.796257439923 \n",
      "\n",
      "AUC:\n",
      "[ 0.6653065   0.66874815  0.67107198  0.66971429  0.66174169  0.67429255\n",
      "  0.66172163  0.66429367  0.67142765  0.66481516] \n",
      "\n",
      "Mean AUC\n",
      "0.667313327537 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.80      1.00      0.89     72246\n",
      "          1       0.34      0.01      0.02     18280\n",
      "\n",
      "avg / total       0.71      0.80      0.71     90526\n",
      " \n",
      "\n",
      "Confusion matrix\n",
      "[[71911   335]\n",
      " [18111   169]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "measure_performance(train_X,train_y,SVM_clf_fitted, show_classification_report=True, show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Should you use accuracy or AUC as a metric for this task?\n",
    "\n",
    "I think we should AUC. Because the data set is strongly biased towards the showing up. If data set is biased we can get good accuracy just using a dummy model which says all patients will show up.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "Based on the above, choose two methods and fit a tuned model:\n",
    "- use 5-fold cross validation for model selection\n",
    "- use 10-fold cross validation for model assessment (based on appropriate performance metric)\n",
    "\n",
    "\n",
    "Report estimated performance for both tuned classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best params:  {'max_features': 15, 'n_estimators': 100}\n",
      "Random Forest Best estimator 0.7201803984727179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [5, 25, 50, 75, 100], 'max_features': [5, 10, 15, 20]},\n",
    "    {'bootstrap': [False], 'n_estimators': [5, 25, 50, 75, 100], 'max_features': [5, 10,15, 20]}\n",
    "]\n",
    "\n",
    "\n",
    "forest_clf = RandomForestClassifier()\n",
    "\n",
    "forest_grid_search = GridSearchCV(forest_clf, param_grid, cv=5,\n",
    "                          scoring=\"roc_auc\")\n",
    "forest_grid_search.fit(train_X, train_y)\n",
    "\n",
    "print (\"Random Forest Best params: \", forest_grid_search.best_params_)\n",
    "print(\"Random Forest Best estimator score {}\".format(forest_grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Linear SVM Best params:  {'C': 0.001}\n",
      "Linear SVM Best estimator score 0.66929850565305\n"
     ]
    }
   ],
   "source": [
    "#from sklearn import svm, grid_search\n",
    "\n",
    "#def svc_param_selection(X, y, nfolds):\n",
    "#    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "#    gammas = [0.001, 0.01, 0.1, 1]\n",
    "#    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "#    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "#    grid_search.fit(X, y)\n",
    "#    grid_search.best_params_\n",
    "#    return grid_search.best_params_\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "SVM_clf = LinearSVC()\n",
    "SVM_grid_search = GridSearchCV(SVM_clf, param_grid, cv=5,\n",
    "                          scoring=\"roc_auc\")\n",
    "SVM_grid_search.fit(train_X,train_y)\n",
    "print (\" Linear SVM Best params: \", SVM_grid_search.best_params_)\n",
    "print(\"Linear SVM Best estimator score {}\".format(SVM_grid_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initialize model parameters w and b\n",
    "# intializing to 0 is not a good idea\n",
    "# it should be a random vector see np.random.randn\n",
    "# YOU NEED TO IMPLEMENT THIS\n",
    "def _initialize_parameters(nfeatures):\n",
    "    w = np.full((nfeatures), np.random.randn())\n",
    "    b = np.full((1), np.random.randn())\n",
    "    return w, b\n",
    "\n",
    "# this is a vectorized version of positive_part operation\n",
    "# we can use this for hinge loss as post_part(1.0 - y*f)\n",
    "pos_part = np.vectorize(lambda u: u if u > 0. else 0.)\n",
    "\n",
    "# compute the value of the linear SVM objective function\n",
    "# given current signed distances, and parameter vector w\n",
    "def _get_objective(f, y, w, lam):\n",
    "    loss = np.sum(pos_part(1.0 - y*f))    \n",
    "    penalty = lam * np.dot(w,w)\n",
    "    return loss + penalty\n",
    "\n",
    "# compute the signed distances\n",
    "# based on current model estimates\n",
    "# w and b\n",
    "# YOU NEED TO IMPLEMENT THIS\n",
    "def _get_signed_distances(X, w, b):\n",
    "    nobs = X.shape[0]\n",
    "    f = np.full(nobs, 0.0)\n",
    "    f = np.multiply(X,w) + b\n",
    "    return f\n",
    "\n",
    "# compute gradients with respect to w and b\n",
    "# YOU NEEED TO IMPLEMENT THIS\n",
    "gradient_part1 = np.vectorize(lambda u: 0. if u >= 1. else -1.)\n",
    "def _get_gradients(f, X, y, w, b, lam):\n",
    "    #nfeatures = X.shape[1]\n",
    "    #gw = np.full((nfeatures), 0.)\n",
    "    #gb = 0.\n",
    "    \n",
    "    yf = y * f\n",
    "    t = gradient_part1(yf)\n",
    "    ty = t * y \n",
    "    gb = np.sum(ty)\n",
    "    \n",
    "    tmp = np.multiply(X.T, ty).T\n",
    "    gw = 2*lam*w + np.sum(tmp , axis = 0)\n",
    "    \n",
    "    return gw, gb\n",
    "\n",
    "# fit an SVM using gradient descent\n",
    "# X: matrix of feature values\n",
    "# y: labels (-1 or 1)\n",
    "# n_iter: numer of iterations\n",
    "# eta: learning rate\n",
    "def fit_svm(X, y, lam, n_iter=100, eta=.4):\n",
    "    nexamples, nfeatures = X.shape\n",
    "    \n",
    "    w, b = _initialize_parameters(nfeatures)\n",
    "    \n",
    "    for k in range(n_iter):\n",
    "        f = _get_signed_distances(X, w, b)\n",
    "        \n",
    "        # print information and \n",
    "        # update the learning rate\n",
    "        if k % 10 == 0:\n",
    "            obj = _get_objective(f, y, w, lam)\n",
    "            eta = eta / 2.0\n",
    "            print(\"it: %d, obj %.2f\" % (k, obj))\n",
    "        \n",
    "        gw, gb = _get_gradients(f, X, y, w, b, lam)\n",
    "        w = w - eta * gw\n",
    "        b = b - eta * b\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (90526,) (90526,101) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-3de66e96b4d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-b7e36bc90198>\u001b[0m in \u001b[0;36mfit_svm\u001b[1;34m(X, y, lam, n_iter, eta)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# update the learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"it: %d, obj %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-b7e36bc90198>\u001b[0m in \u001b[0;36m_get_objective\u001b[1;34m(f, y, w, lam)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# given current signed distances, and parameter vector w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mpenalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (90526,) (90526,101) "
     ]
    }
   ],
   "source": [
    "w,b = fit_svm(train_X, train_y, 1.0, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initialize model parameters w and b\n",
    "# intializing to 0 is not a good idea\n",
    "# it should be a random vector see np.random.randn\n",
    "# YOU NEED TO IMPLEMENT THIS\n",
    "def _initialize_parameters(nfeatures):\n",
    "    w = np.full((nfeatures), 0.0)\n",
    "    b = np.full((1), 0.0)\n",
    "    return w, b\n",
    "\n",
    "# this is a vectorized version of positive_part operation\n",
    "# we can use this for hinge loss as post_part(1.0 - y*f)\n",
    "pos_part = np.vectorize(lambda u: u if u > 0. else 0.)\n",
    "\n",
    "# compute the value of the linear SVM objective function\n",
    "# given current signed distances, and parameter vector w\n",
    "def _get_objective(f, y, w, lam):\n",
    "    loss = np.sum(pos_part(1.0 - y*f))\n",
    "    penalty = lam * np.dot(w,w)\n",
    "    return loss + penalty\n",
    "\n",
    "# compute the signed distances\n",
    "# based on current model estimates\n",
    "# w and b\n",
    "# YOU NEED TO IMPLEMENT THIS\n",
    "def _get_signed_distances(X, w, b):\n",
    "    nobs = X.shape[0]\n",
    "    f = np.full(nobs, 0.0)\n",
    "    return f\n",
    "\n",
    "# compute gradients with respect to w and b\n",
    "# YOU NEEED TO IMPLEMENT THIS\n",
    "def _get_gradients(f, X, y, w, b, lam):\n",
    "    nfeatures = X.shape[1]\n",
    "    gw = np.full((nfeatures), 0.)\n",
    "    gb = 0.\n",
    "    return gw, gb\n",
    "\n",
    "# fit an SVM using gradient descent\n",
    "# X: matrix of feature values\n",
    "# y: labels (-1 or 1)\n",
    "# n_iter: numer of iterations\n",
    "# eta: learning rate\n",
    "def fit_svm(X, y, lam, n_iter=100, eta=.4):\n",
    "    nexamples, nfeatures = X.shape\n",
    "    \n",
    "    w, b = _initialize_parameters(nfeatures)\n",
    "    \n",
    "    for k in range(n_iter):\n",
    "        f = _get_signed_distances(X, w, b)\n",
    "        \n",
    "        # print information and \n",
    "        # update the learning rate\n",
    "        if k % 10 == 0:\n",
    "            obj = _get_objective(f, y, w, lam)\n",
    "            eta = eta / 2.0\n",
    "            print(\"it: %d, obj %.2f\" % (k, obj))\n",
    "        \n",
    "        gw, gb = _get_gradients(f, X, y, w, b, lam)\n",
    "        w = w - eta * gw\n",
    "        b = b - eta * b\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0, obj 90526.00\n",
      "it: 10, obj 90526.00\n",
      "it: 20, obj 90526.00\n",
      "it: 30, obj 90526.00\n",
      "it: 40, obj 90526.00\n",
      "it: 50, obj 90526.00\n",
      "it: 60, obj 90526.00\n",
      "it: 70, obj 90526.00\n",
      "it: 80, obj 90526.00\n",
      "it: 90, obj 90526.00\n"
     ]
    }
   ],
   "source": [
    "w,b = fit_svm(train_X, train_y, 1.0, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
